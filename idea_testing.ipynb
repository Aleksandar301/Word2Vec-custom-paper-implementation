{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9997ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def load_and_clean_text(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Remove Gutenberg header\n",
    "    start_marker = \"*** START OF\"\n",
    "    end_marker = \"*** END OF\"\n",
    "    \n",
    "    start_idx = text.find(start_marker)\n",
    "    end_idx = text.find(end_marker)\n",
    "    \n",
    "    if start_idx != -1 and end_idx != -1:\n",
    "        text = text[start_idx:end_idx]\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Keep only letters and spaces\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def build_vocab(tokens, min_count=2):\n",
    "    word_counts = Counter(tokens)\n",
    "\n",
    "    # Remove rare words\n",
    "    vocab_words = [word for word, count in word_counts.items() if count >= min_count]\n",
    "\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "def tokens_to_indices(tokens, word_to_idx):\n",
    "    return [word_to_idx[word] for word in tokens if word in word_to_idx]\n",
    "\n",
    "def build_negative_sampling_dist(tokens, word_to_idx):\n",
    "    word_counts = Counter(tokens)\n",
    "\n",
    "    vocab_size = len(word_to_idx)\n",
    "    freqs = np.zeros(vocab_size)\n",
    "\n",
    "    for word, idx in word_to_idx.items():\n",
    "        freqs[idx] = word_counts[word]\n",
    "\n",
    "    # Apply 3/4 power\n",
    "    freqs = freqs ** 0.75\n",
    "    freqs /= np.sum(freqs)\n",
    "\n",
    "    return freqs\n",
    "\n",
    "def preprocess(filepath, min_count=2):\n",
    "    text = load_and_clean_text(filepath)\n",
    "    tokens = tokenize(text)\n",
    "\n",
    "    word_to_idx, idx_to_word = build_vocab(tokens, min_count)\n",
    "    corpus_indices = tokens_to_indices(tokens, word_to_idx)\n",
    "\n",
    "    neg_sampling_dist = build_negative_sampling_dist(tokens, word_to_idx)\n",
    "\n",
    "    return corpus_indices, word_to_idx, idx_to_word, neg_sampling_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8425f97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (in tokens): 9382\n",
      "Vocabulary size: 816\n"
     ]
    }
   ],
   "source": [
    "corpus, word_to_idx, idx_to_word, neg_dist = preprocess(\"alice.txt\")\n",
    "print(f\"Corpus length (in tokens): {len(corpus)}\")\n",
    "print(f\"Vocabulary size: {len(word_to_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433d8acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus saved to 'corpus_output.txt'\n",
      "File preview (first 200 chars): of the alice s adventures in wonderland illustration alice in the room of the duchess the alice s adventures in wonderland sam l gabriel sons company new york by sam l gabriel sons company new york al\n"
     ]
    }
   ],
   "source": [
    "# Save corpus to file\n",
    "corpus_words = [idx_to_word[idx] for idx in corpus]\n",
    "corpus_text = \" \".join(corpus_words)\n",
    "\n",
    "with open(\"corpus_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(corpus_text)\n",
    "\n",
    "print(f\"Corpus saved to 'corpus_output.txt'\")\n",
    "print(f\"File preview (first 200 chars): {corpus_text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a8ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbow_samples(corpus_indices, window_size=2):\n",
    "    \"\"\"\n",
    "    Generates CBOW training samples.\n",
    "\n",
    "    Parameters:\n",
    "        corpus_indices (list): List of word indices\n",
    "        window_size (int): Context window size (c)\n",
    "\n",
    "    Returns:\n",
    "        samples (list of tuples):\n",
    "            Each element is (context_indices, target_index)\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = []\n",
    "    T = len(corpus_indices)\n",
    "\n",
    "    for t in range(window_size, T - window_size):\n",
    "        \n",
    "        # Target word w_O\n",
    "        target = corpus_indices[t]\n",
    "\n",
    "        # Context words w_1 ... w_C\n",
    "        context = []\n",
    "\n",
    "        # Left context\n",
    "        for i in range(t - window_size, t):\n",
    "            context.append(corpus_indices[i])\n",
    "\n",
    "        # Right context\n",
    "        for i in range(t + 1, t + window_size + 1):\n",
    "            context.append(corpus_indices[i])\n",
    "\n",
    "        samples.append((context, target))\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f45f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9378\n",
      "Example sample: ([0, 1, 3, 4], 2)\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "training_samples = generate_cbow_samples(corpus, window_size)\n",
    "\n",
    "print(\"Number of training samples:\", len(training_samples))\n",
    "print(\"Example sample:\", training_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fcdbf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples saved to 'training_samples.txt' (9378 samples)\n"
     ]
    }
   ],
   "source": [
    "with open(\"training_samples.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, (context, target) in enumerate(training_samples):\n",
    "        # Convert indices to words\n",
    "        context_words = [idx_to_word[idx] for idx in context]\n",
    "        target_word = idx_to_word[target]\n",
    "        \n",
    "        # Format: context_words | target_word\n",
    "        line = f\"{' '.join(context_words)} | {target_word}\\n\"\n",
    "        f.write(line)\n",
    "\n",
    "print(f\"Training samples saved to 'training_samples.txt' ({len(training_samples)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66db2b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique context sizes: {4}\n",
      "All contexts are size 4: True\n",
      "Min size: 4, Max size: 4\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: verify all context sizes are exactly 4 words\n",
    "context_sizes = [len(context) for context, target in training_samples]\n",
    "unique_sizes = set(context_sizes)\n",
    "\n",
    "print(f\"Unique context sizes: {unique_sizes}\")\n",
    "print(f\"All contexts are size 4: {unique_sizes == {4}}\")\n",
    "print(f\"Min size: {min(context_sizes)}, Max size: {max(context_sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f637c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter setup\n",
    "V = len(word_to_idx)\n",
    "N = 50\n",
    "\n",
    "W = np.random.randn(V, N) * 0.01\n",
    "W_prime = np.random.randn(V, N) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a7edf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative sampling setup\n",
    "def sample_negative_words(K, neg_dist):\n",
    "    return np.random.choice(len(neg_dist), size=K, p=neg_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30f1017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow(training_samples, W, W_prime, neg_dist, \n",
    "               window_size=2, N=50, K=5, lr=0.025, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train CBOW with Negative Sampling in pure NumPy.\n",
    "    \n",
    "    Parameters:\n",
    "        training_samples : list of tuples\n",
    "            Each tuple: (context_indices, target_index)\n",
    "        W : np.ndarray\n",
    "            Input embedding matrix (V, N)\n",
    "        W_prime : np.ndarray\n",
    "            Output embedding matrix (V, N)\n",
    "        neg_dist : np.ndarray\n",
    "            Negative sampling distribution (length V)\n",
    "        window_size : int\n",
    "            Context window size\n",
    "        N : int\n",
    "            Embedding dimension\n",
    "        K : int\n",
    "            Number of negative samples\n",
    "        lr : float\n",
    "            Learning rate\n",
    "        num_epochs : int\n",
    "            Number of training epochs\n",
    "\n",
    "    Returns:\n",
    "        W, W_prime : np.ndarrays\n",
    "            Updated embedding matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    V = W.shape[0]\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sample_negative_words(K, neg_dist):\n",
    "        return np.random.choice(len(neg_dist), size=K, p=neg_dist)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for context_indices, target in training_samples:\n",
    "            \n",
    "            # Forward Pass \n",
    "            v_context = W[context_indices]                 # (C, N)\n",
    "            h = np.mean(v_context, axis=0)                # (N,)\n",
    "            \n",
    "            u_target = W_prime[target]                     # (N,)\n",
    "            neg_indices = sample_negative_words(K, neg_dist)\n",
    "            u_neg = W_prime[neg_indices]                  # (K, N)\n",
    "            \n",
    "            # Scores\n",
    "            s_pos = np.dot(u_target, h)\n",
    "            s_neg = np.dot(u_neg, h)                      # (K,)\n",
    "            \n",
    "            # Sigmoid\n",
    "            sig_pos = sigmoid(s_pos)\n",
    "            sig_neg = sigmoid(-s_neg)\n",
    "            \n",
    "            # Loss \n",
    "            loss = -np.log(sig_pos) - np.sum(np.log(sig_neg))\n",
    "            total_loss += loss\n",
    "            \n",
    "            # Backward Pass \n",
    "            delta_pos = sig_pos - 1                        # scalar\n",
    "            delta_neg = 1 - sig_neg                        # (K,)\n",
    "            \n",
    "            # Gradients for output embeddings\n",
    "            grad_u_target = delta_pos * h                  # (N,)\n",
    "            grad_u_neg = delta_neg[:, None] * h           # (K, N)\n",
    "            \n",
    "            # Gradient wrt hidden layer\n",
    "            grad_h = delta_pos * u_target + np.dot(delta_neg, u_neg)\n",
    "            \n",
    "            # Distribute to context embeddings\n",
    "            grad_v_context = grad_h / len(context_indices)\n",
    "            \n",
    "            # Parameter Updates \n",
    "            W_prime[target] -= lr * grad_u_target\n",
    "            W_prime[neg_indices] -= lr * grad_u_neg\n",
    "            \n",
    "            for idx in context_indices:\n",
    "                W[idx] -= lr * grad_v_context\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    return W, W_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32db39e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 38965.8890\n",
      "Epoch 2/20, Loss: 36325.7317\n",
      "Epoch 3/20, Loss: 30572.7817\n",
      "Epoch 4/20, Loss: 28093.6809\n",
      "Epoch 5/20, Loss: 27104.9220\n",
      "Epoch 6/20, Loss: 26510.1067\n",
      "Epoch 7/20, Loss: 26027.2008\n",
      "Epoch 8/20, Loss: 25607.9288\n",
      "Epoch 9/20, Loss: 25211.3196\n",
      "Epoch 10/20, Loss: 24814.3068\n",
      "Epoch 11/20, Loss: 24440.9758\n",
      "Epoch 12/20, Loss: 24031.3123\n",
      "Epoch 13/20, Loss: 23649.7937\n",
      "Epoch 14/20, Loss: 23264.2788\n",
      "Epoch 15/20, Loss: 22913.9350\n",
      "Epoch 16/20, Loss: 22518.2563\n",
      "Epoch 17/20, Loss: 22174.7185\n",
      "Epoch 18/20, Loss: 21864.0546\n",
      "Epoch 19/20, Loss: 21520.1256\n",
      "Epoch 20/20, Loss: 21175.5014\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "N = 50\n",
    "K = 5\n",
    "lr = 0.025\n",
    "num_epochs = 20\n",
    "\n",
    "# Train embeddings\n",
    "W, W_prime = train_cbow(training_samples, W, W_prime, neg_dist,\n",
    "                        window_size=2, N=N, K=K, lr=lr, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b71d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e51c5856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice <-> queen: 0.5106595509137505\n",
      "alice <-> rabbit: 0.5198968419112165\n"
     ]
    }
   ],
   "source": [
    "alice_idx = word_to_idx['alice']\n",
    "queen_idx = word_to_idx['queen']\n",
    "rabbit_idx = word_to_idx['rabbit']\n",
    "\n",
    "v_alice = W[alice_idx]\n",
    "v_queen = W[queen_idx]\n",
    "v_rabbit = W[rabbit_idx]\n",
    "\n",
    "print(\"alice <-> queen:\", cosine_similarity(v_alice, v_queen))\n",
    "print(\"alice <-> rabbit:\", cosine_similarity(v_alice, v_rabbit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d390596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "20 COSINE SIMILARITY EXAMPLES FOR VERIFICATION\n",
      "======================================================================\n",
      " 1. fur             <-> adventures      :  0.9043\n",
      " 2. adventures      <-> without         :  0.8447\n",
      " 3. without         <-> so              :  0.8765\n",
      " 4. so              <-> it              :  0.7649\n",
      " 5. it              <-> will            :  0.4264\n",
      " 6. will            <-> notice          :  0.9169\n",
      " 7. notice          <-> hardly          :  0.6140\n",
      " 8. hardly          <-> shorter         :  0.8250\n",
      " 9. shorter         <-> jurymen         :  0.6567\n",
      "10. jurymen         <-> under           :  0.8435\n",
      "11. under           <-> leave           :  0.7832\n",
      "12. leave           <-> sure            :  0.4329\n",
      "13. sure            <-> a               :  0.2630\n",
      "14. a               <-> pig             :  0.1510\n",
      "15. pig             <-> beg             :  0.1937\n",
      "16. beg             <-> end             :  0.4024\n",
      "17. end             <-> moment          :  0.8566\n",
      "18. moment          <-> felt            :  0.9411\n",
      "19. felt            <-> most            :  0.7211\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate 20 cosine similarity examples for verification\n",
    "import random\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_u == 0 or norm_v == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm_u * norm_v)\n",
    "\n",
    "# Get a sample of words from vocabulary\n",
    "sample_words = random.sample(list(word_to_idx.keys()), min(20, len(word_to_idx)))\n",
    "sample_indices = [word_to_idx[w] for w in sample_words]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"20 COSINE SIMILARITY EXAMPLES FOR VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "examples = []\n",
    "for i in range(min(20, len(sample_indices) - 1)):\n",
    "    idx1 = sample_indices[i]\n",
    "    idx2 = sample_indices[i + 1]\n",
    "    \n",
    "    word1 = idx_to_word[idx1]\n",
    "    word2 = idx_to_word[idx2]\n",
    "    \n",
    "    v1 = W[idx1]\n",
    "    v2 = W[idx2]\n",
    "    \n",
    "    similarity = cosine_similarity(v1, v2)\n",
    "    examples.append((word1, word2, similarity))\n",
    "    \n",
    "    print(f\"{i+1:2d}. {word1:15s} <-> {word2:15s} : {similarity:7.4f}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
